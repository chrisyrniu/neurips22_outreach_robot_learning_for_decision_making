{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/chrisyrniu/neurips22_outreach_robot_learning_for_decision_making/blob/main/robot_learning_for_decision_making.ipynb","timestamp":1669385383152}],"authorship_tag":"ABX9TyNTCvXAXqIuHxzGMcXuZmnV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Installation**"],"metadata":{"id":"_HLFkkGxyu5R"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cLNyAT5yEZ2"},"outputs":[],"source":["#@title Mount your Google Drive\n","#@markdown Your work will be stored in a folder called `neurips22_outreach_rl4dm`\n","\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["#@title Set up mount symlink\n","\n","DRIVE_PATH = '/content/gdrive/My\\ Drive/neurips22_outreach_rl4dm'\n","DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","if not os.path.exists(DRIVE_PYTHON_PATH):\n","  %mkdir $DRIVE_PATH\n","\n","## the space in `My Drive` causes some issues,\n","## make a symlink to avoid this\n","SYM_PATH = '/content/neurips22_outreach_rl4dm'\n","if not os.path.exists(SYM_PATH):\n","  !ln -s $DRIVE_PATH $SYM_PATH"],"metadata":{"id":"FeNjcX8NEPnQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title apt install requirements\n","!apt update \n","!apt install -y --no-install-recommends \\\n","        build-essential \\\n","        curl \\\n","        git \\\n","        gnupg2 \\\n","        make \\\n","        cmake \\\n","        ffmpeg \\\n","        swig \\\n","        libz-dev \\\n","        unzip \\\n","        zlib1g-dev \\\n","        libglfw3 \\\n","        libglfw3-dev \\\n","        libxrandr2 \\\n","        libxinerama-dev \\\n","        libxi6 \\\n","        libxcursor-dev \\\n","        libgl1-mesa-dev \\\n","        libgl1-mesa-glx \\\n","        libglew-dev \\\n","        libosmesa6-dev \\\n","        lsb-release \\\n","        ack-grep \\\n","        patchelf \\\n","        wget \\\n","        xpra \\\n","        xserver-xorg-dev \\\n","        xvfb \\\n","        python-opengl \\\n","        ffmpeg"],"metadata":{"id":"OdbaSLrUJnc6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Clone the repo\n","\n","%cd $SYM_PATH\n","!git clone https://github.com/chrisyrniu/neurips22_outreach_robot_learning_for_decision_making.git\n","%cd neurips22_outreach_robot_learning_for_decision_making\n","%pip install -r requirements_colab.txt\n","%pip install gym[box2d]==0.25.2"],"metadata":{"id":"Jzv6kFYVLL8y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Set up virtual display\n","\n","from pyvirtualdisplay import Display\n","\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"metadata":{"id":"vlftbT3e3FLN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train an agent with reinforcement learning in several minutes!"],"metadata":{"id":"zoPZ0DH8qK-V"}},{"cell_type":"code","source":["#@title First, let's visualize a random agent before training\n","#@markdown The cheetah can barely move forward!\n","\n","import gym\n","from colab_utils import (\n","    wrap_env,\n","    show_video\n",")\n","\n","env_name = \"HalfCheetah-v4\"\n","env = wrap_env(gym.make(env_name, render_mode='rgb_array'))\n","\n","observation = env.reset()\n","for i in range(400):\n","    env.render()\n","    obs, rew, term, _ = env.step(env.action_space.sample() ) \n","    if term:\n","      break;\n","            \n","env.close()\n","print('Loading video...')\n","show_video()"],"metadata":{"id":"pdYet0-iSwXM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Then, we will use reinforcement learning to learn to control each joint of the cheetah and make it run!\n","#@markdown Let's tweak some parameters that impact the learning performance.\n","\n","#@markdown Learning rate represents the step size when you update your model. Your model will learn nothing when your step is too small or too large!\n","#@markdown You might want to pick a learning rate from {1, 0.1, 0.01, 0.001, 0.0001}.\n","learning_rate = 0.001 #@param\n","\n","#@markdown After each training epoch, your model will be evaluated and you can read the evaluation results!\n","#@markdown You could set it as 2 or 3 when you tune the learning rate.\n","#@markdown After finding a good learning rate, you could let the trainig run more epochs to achieve a better performance!\n","num_epochs = 3 #@param\n","\n","import torch\n","cuda = torch.cuda.is_available()"],"metadata":{"id":"G8lm55Z8tHAH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Now, the training program is ready to set off! We will use the Soft Actor-Critic algorithm.\n","#@markdown Please note that the random agent normally gets an episode reward around worse than -100.\n","!python run_sac.py --task $env_name --actor-lr $learning_rate --critic-lr $learning_rate --epoch $num_epochs"],"metadata":{"id":"YdG8B-AybNfi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Let's visualize your trained agent!\n","#@markdown The best model was automatically saved for you during training, and you can find it in the \"/log\" folder. The model file name is \"policy.pth\".\n","\n","#@markdown Please copy the path to your model to the following blank (e.g., /content/neurips22_outreach_rl4dm/neurips22_outreach_robot_learning_for_decision_making/log/HalfCheetah-v4/sac/0/221125-112556/policy.pth).\n","\n","model_path = \"/content/neurips22_outreach_rl4dm/neurips22_outreach_robot_learning_for_decision_making/log/HalfCheetah-v4/sac/0/221125-135305/policy.pth\" #@param {type: \"string\"}\n","#@markdown The following code will automatically pick\n","\n","from load_sac import load_sac\n","from tianshou.data import Batch, to_numpy\n","import numpy as np\n","\n","if cuda:\n","  device = 'cuda'\n","else:\n","  device = 'cpu'\n","policy = load_sac(model_path, env_name, device)\n","env = wrap_env(gym.make(env_name, render_mode='rgb_array'))\n","rewards = []\n","\n","obs = env.reset()\n","for i in range(400):\n","    obs = np.array(obs).reshape(1, -1)\n","    obs = Batch(obs=obs, info=obs)\n","    result = policy(obs)\n","    act = to_numpy(result.act)\n","    act = policy.map_action(act).reshape(-1)\n","    obs, rew, term, _ = env.step(act) \n","    rewards.append(rew)\n","    if term:\n","      break\n","\n","# print('tested single episode reward', np.array(rewards).sum())\n","env.close()\n","print('Loading video...')\n","show_video()\n","\n"],"metadata":{"id":"S6cx7EpQpfgr"},"execution_count":null,"outputs":[]}]}