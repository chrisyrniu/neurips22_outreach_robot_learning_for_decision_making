{"cells":[{"cell_type":"markdown","metadata":{"id":"_HLFkkGxyu5R"},"source":["# **Installation**"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","collapsed":true,"id":"OdbaSLrUJnc6"},"outputs":[],"source":["#@title Install system requirements\n","%%capture\n","!apt update \n","!apt install -y --no-install-recommends \\\n","        build-essential \\\n","        curl \\\n","        git \\\n","        gnupg2 \\\n","        make \\\n","        cmake \\\n","        ffmpeg \\\n","        swig \\\n","        libz-dev \\\n","        unzip \\\n","        zlib1g-dev \\\n","        libglfw3 \\\n","        libglfw3-dev \\\n","        libxrandr2 \\\n","        libxinerama-dev \\\n","        libxi6 \\\n","        libxcursor-dev \\\n","        libgl1-mesa-dev \\\n","        libgl1-mesa-glx \\\n","        libglew-dev \\\n","        libosmesa6-dev \\\n","        lsb-release \\\n","        ack-grep \\\n","        patchelf \\\n","        wget \\\n","        xpra \\\n","        xserver-xorg-dev \\\n","        xvfb \\\n","        python-opengl \\\n","        ffmpeg"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Jzv6kFYVLL8y"},"outputs":[],"source":["#@title Clone the repo and install\n","%%capture\n","import os\n","SYM_PATH = '/content/neurips22_outreach_rl4dm'\n","if not os.path.exists(SYM_PATH):\n","  %mkdir $SYM_PATH\n","%cd $SYM_PATH\n","!git clone https://github.com/chrisyrniu/neurips22_outreach_robot_learning_for_decision_making.git\n","%cd neurips22_outreach_robot_learning_for_decision_making\n","%pip install -r requirements_colab.txt\n","%pip install gym[box2d]==0.25.2"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"vlftbT3e3FLN"},"outputs":[],"source":["#@title Set up virtual display\n","%%capture\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"]},{"cell_type":"markdown","metadata":{"id":"zoPZ0DH8qK-V"},"source":["# **Train an agent with reinforcement learning in several minutes!**"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"pdYet0-iSwXM"},"outputs":[],"source":["#@title First, let's visualize a random agent before training.\n","#@markdown The cheetah can barely move forward!\n","\n","import gym\n","from colab_utils import (\n","    wrap_env,\n","    show_video\n",")\n","\n","env_name = \"HalfCheetah-v4\"\n","env = wrap_env(gym.make(env_name, render_mode='rgb_array'))\n","\n","observation = env.reset()\n","for i in range(200):\n","    env.render()\n","    obs, rew, term, _ = env.step(env.action_space.sample() ) \n","    if term:\n","      break;\n","            \n","env.close()\n","print('Loading video...')\n","show_video()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"G8lm55Z8tHAH"},"outputs":[],"source":["#@title Then, we will use reinforcement learning to learn to control each joint of the cheetah and make it run!\n","#@markdown Let's tweak some parameters that impact the learning performance.\n","\n","#@markdown Learning rate represents the \"updating speed\" of your model. \n","#@markdown Your model will learn nothing when your learning rate is too small or too large!\n","#@markdown You might want to pick a learning rate from {0.01, 0.001, 0.0001, 0.00001}.\n","learning_rate = 0.001 #@param\n","\n","#@markdown Here, each epoch includes 5000 environment steps.\n","#@markdown For each step, the agent will receive an observation, execute an action, and receive a reward signal.\n","#@markdown After each training epoch, your model will be evaluated and you can read the evaluation results!\n","\n","#@markdown You could set it as 2 or 3 when you tune the learning rate.\n","#@markdown After finding a good learning rate, you could let the trainig run more epochs to achieve a better performance!\n","num_epochs = 3 #@param"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YdG8B-AybNfi"},"outputs":[],"source":["#@title Now, the training program is ready to set off! We will use the Soft Actor-Critic algorithm.\n","#@markdown We will use tensorboard to monitor the training process. \n","\n","#@markdown You could check \"test/reward\" or \"train/reward\" to tell if this run is good!\n","%load_ext tensorboard\n","%tensorboard --logdir log"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"t3rUUKPu_vb0"},"outputs":[],"source":["#@markdown Click here to start training!\n","\n","#@markdown You could also directly read information from the current run in the output lines.\n","import torch\n","cuda = torch.cuda.is_available()\n","print('Use GPU') if cuda else print('Use CPU')\n","!python run_sac.py --task $env_name --actor-lr $learning_rate --critic-lr $learning_rate --epoch $num_epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"S6cx7EpQpfgr"},"outputs":[],"source":["#@title Let's visualize your trained agent!\n","#@markdown The best model was automatically saved for you during training, and you can find it in the \"/log\" folder. The model file name is \"policy.pth\".\n","\n","#@markdown Please copy the path to your model to the following blank.\n","\n","model_path = \"\" #@param {type: \"string\"}\n","\n","from load_sac import load_sac\n","from tianshou.data import Batch, to_numpy\n","import numpy as np\n","import torch\n","cuda = torch.cuda.is_available()\n","if cuda:\n","  device = 'cuda'\n","else:\n","  device = 'cpu'\n","policy = load_sac(model_path, env_name, device)\n","env = wrap_env(gym.make(env_name, render_mode='rgb_array'))\n","rewards = []\n","\n","obs = env.reset()\n","for i in range(200):\n","    obs = np.array(obs).reshape(1, -1)\n","    obs = Batch(obs=obs, info=obs)\n","    result = policy(obs)\n","    act = to_numpy(result.act)\n","    act = policy.map_action(act).reshape(-1)\n","    obs, rew, term, _ = env.step(act) \n","    rewards.append(rew)\n","    if term:\n","      break\n","\n","# print('tested single episode reward', np.array(rewards).sum())\n","env.close()\n","print('Loading video...')\n","show_video()\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPZrIp0vK+wCGb+8+1S19ot","provenance":[{"file_id":"https://github.com/chrisyrniu/neurips22_outreach_robot_learning_for_decision_making/blob/main/robot_learning_for_decision_making.ipynb","timestamp":1669385383152}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
